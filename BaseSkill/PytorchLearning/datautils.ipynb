{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ~.Dataset()和~.Dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在pytorch中，torch.utils.data.里面提供方面的数据集和数据迭代器\n",
    "在每次训练中我们可以使用这个迭代器输出每一个batchsize的数据，并能够在输出之前及时对数据做预处理或者数据增强的操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~.Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-2.0204, -1.8964]), tensor(-0.2659))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TensorDataset(Dataset):\n",
    "    '''\n",
    "    TD继承dataset，重载init、getitem、len操作\n",
    "    实现将一组tensor封装成为tensor数据集\n",
    "    通过index可以得到数据集的数据\n",
    "    通过len得到数据集的大小\n",
    "    '''\n",
    "    def __init__(self,x,y):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "        return self.x[index],self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.size(0)\n",
    "# 看我们如何使用这个class\n",
    "x=torch.randn(120,2)/2-2\n",
    "y=torch.randn(120)\n",
    "# 封装成为dataset\n",
    "t_dataset=TensorDataset(x,y)\n",
    "# 抽取数据\n",
    "t_dataset[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_dataset.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~.Dataloder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataloader将dataset对象或者自定义数据类的对象封装成为一个迭代器，迭代器可以迭代输出dataset的内容，可以实现多进程、shuffle、sample、校对等多种操作\n",
    "__init__()输入\n",
    "1. dataset\n",
    "2. batch_size()\n",
    "3. shuffle\n",
    "4. collate_fn 处理不同情况下输入dataset的封装，一般默认即可\n",
    "5. batch_sampler 一般采取默认\n",
    "6. sampler 与shuffle互斥，默认即可\n",
    "6. num_workers 线程数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.5042, -2.1659],\n",
      "        [-1.8833, -1.1500],\n",
      "        [-1.9991, -2.1198],\n",
      "        [-1.8401, -1.9984],\n",
      "        [-1.6414, -2.1670],\n",
      "        [-1.9247, -1.8506],\n",
      "        [-1.1528, -1.9911],\n",
      "        [-1.7750, -1.8837]]) tensor([-1.5444,  1.9749, -0.2567, -0.6198,  0.3054,  0.2757, -0.2918, -0.6072])\n",
      "tensor([[-1.8771, -1.9792],\n",
      "        [-1.9391, -2.5095],\n",
      "        [-2.2484, -1.9347],\n",
      "        [-2.2756, -1.2855],\n",
      "        [-2.6127, -1.9398],\n",
      "        [-2.2896, -1.8757],\n",
      "        [-1.9209, -1.5757],\n",
      "        [-1.8822, -1.7244]]) tensor([-0.3374,  0.0069,  0.9390, -1.4069,  1.0069, -0.6879,  0.9389, -0.1247])\n",
      "tensor([[-2.2115, -1.6545],\n",
      "        [-1.8845, -1.9511],\n",
      "        [-2.3268, -1.4210],\n",
      "        [-2.5839, -2.7000],\n",
      "        [-2.1112, -1.4714],\n",
      "        [-2.0672, -3.4426],\n",
      "        [-2.3766, -1.9899],\n",
      "        [-2.0631, -2.6673]]) tensor([-2.1607,  0.7821, -0.3751,  0.3690,  0.9473, -0.3641,  0.6716, -1.9142])\n",
      "tensor([[-2.5535, -1.9998],\n",
      "        [-2.1357, -1.8139],\n",
      "        [-1.9401, -2.0147],\n",
      "        [-2.1395, -2.9639],\n",
      "        [-2.1854, -2.6044],\n",
      "        [-1.9395, -2.2652],\n",
      "        [-2.0863, -1.3758],\n",
      "        [-1.7809, -2.2854]]) tensor([ 0.6919, -1.6602, -0.0360,  1.1805, -0.0825, -1.9503, -0.0217, -0.4334])\n",
      "tensor([[-1.8647, -3.0431],\n",
      "        [-2.4654, -2.4857],\n",
      "        [-2.3064, -2.2113],\n",
      "        [-2.5694, -1.8254],\n",
      "        [-3.2456, -1.8478],\n",
      "        [-2.3033, -1.4112],\n",
      "        [-1.9343, -2.0185],\n",
      "        [-1.6379, -1.3595]]) tensor([ 0.4497, -0.2822,  0.1549,  1.8175,  0.7075, -0.4093,  0.3664,  0.2791])\n",
      "tensor([[-2.0036, -2.5935],\n",
      "        [-1.3779, -2.4934],\n",
      "        [-1.8273, -1.7451],\n",
      "        [-2.2988, -2.5410],\n",
      "        [-1.4882, -2.5697],\n",
      "        [-1.4787, -2.9676],\n",
      "        [-2.9861, -1.8373],\n",
      "        [-2.7715, -3.1632]]) tensor([-1.7047,  0.8433,  0.3806, -0.1399,  0.7600,  0.9947, -0.2638, -1.1962])\n",
      "tensor([[-1.9445, -2.1617],\n",
      "        [-1.6839, -2.2464],\n",
      "        [-2.3671, -2.8055],\n",
      "        [-2.8789, -2.2202],\n",
      "        [-2.1835, -1.5201],\n",
      "        [-2.1368, -1.7777],\n",
      "        [-1.3326, -1.7805],\n",
      "        [-2.3744, -2.8454]]) tensor([-0.8207, -0.4773, -0.3115, -0.5244, -0.6627,  0.2634, -0.7460,  1.6954])\n",
      "tensor([[-1.6448, -1.6690],\n",
      "        [-2.8116, -2.2269],\n",
      "        [-2.8573, -1.7726],\n",
      "        [-2.2467, -1.5932],\n",
      "        [-1.5796, -1.8360],\n",
      "        [-1.4636, -1.8410],\n",
      "        [-1.7969, -1.1309],\n",
      "        [-1.6020, -1.9668]]) tensor([-0.5218, -0.7519, -0.9326,  1.0675,  0.2325,  0.2971, -1.2087,  0.9505])\n",
      "tensor([[-2.7413, -1.5559],\n",
      "        [-1.6659, -1.3222],\n",
      "        [-2.2175, -1.3344],\n",
      "        [-1.7705, -2.0864],\n",
      "        [-1.7176, -1.8547],\n",
      "        [-2.5835, -1.2701],\n",
      "        [-2.0521, -2.1510],\n",
      "        [-1.5530, -1.8215]]) tensor([-0.2778, -0.7038, -0.3302, -0.8759, -0.5648, -0.1529,  0.6714, -0.3105])\n",
      "tensor([[-1.6945, -1.8686],\n",
      "        [-2.4466, -1.0348],\n",
      "        [-2.0847, -1.7937],\n",
      "        [-1.1005, -1.6122],\n",
      "        [-2.5216, -1.0263],\n",
      "        [-2.2092, -1.1999],\n",
      "        [-2.2230, -1.9038],\n",
      "        [-1.8181, -2.5934]]) tensor([-0.8642,  0.7461, -1.0096, -0.3762,  1.1784,  0.1619,  0.1639, -0.7127])\n",
      "tensor([[-1.9860, -1.8712],\n",
      "        [-1.7449, -1.5229],\n",
      "        [-1.8716, -3.1200],\n",
      "        [-2.1859, -2.3041],\n",
      "        [-2.1635, -1.4105],\n",
      "        [-2.0806, -1.7183],\n",
      "        [-2.5385, -1.2260],\n",
      "        [-2.3197, -1.7639]]) tensor([ 0.1887, -0.2597, -0.1013, -0.2097,  0.1689, -0.1557, -0.6857, -0.1067])\n",
      "tensor([[-1.3603, -1.9062],\n",
      "        [-1.8368, -1.6025],\n",
      "        [-1.3763, -1.6397],\n",
      "        [-2.3859, -2.3226],\n",
      "        [-2.5249, -1.2029],\n",
      "        [-1.4925, -2.7959],\n",
      "        [-1.7101, -2.0545],\n",
      "        [-2.0838, -2.2380]]) tensor([-0.9677, -1.5011, -1.3745, -0.5350, -0.0498, -1.3529, -0.6354,  1.4062])\n",
      "tensor([[-2.2854, -2.2348],\n",
      "        [-2.0042, -1.5531],\n",
      "        [-2.9998, -2.0088],\n",
      "        [-2.0204, -1.8964],\n",
      "        [-1.5697, -2.6358],\n",
      "        [-1.7670, -1.6146],\n",
      "        [-2.1415, -1.7702],\n",
      "        [-1.5256, -2.2568]]) tensor([ 8.2117e-01, -8.9192e-01,  9.7355e-04, -2.6589e-01, -7.8327e-01,\n",
      "        -5.7698e-01, -3.6759e-01, -1.7575e+00])\n",
      "tensor([[-2.8034, -1.7500],\n",
      "        [-2.2624, -2.3859],\n",
      "        [-2.0737, -1.5847],\n",
      "        [-2.9372, -1.8822],\n",
      "        [-2.8910, -2.2912],\n",
      "        [-2.2798, -1.6485],\n",
      "        [-2.1706, -2.3215],\n",
      "        [-1.2932, -1.6964]]) tensor([-0.4273,  1.5056,  2.3093, -0.8747,  0.2384,  0.1602,  2.4493, -0.2514])\n",
      "tensor([[-1.6930, -2.2296],\n",
      "        [-1.4217, -2.0880],\n",
      "        [-2.5020, -1.7281],\n",
      "        [-1.7695, -2.1868],\n",
      "        [-0.8932, -1.4880],\n",
      "        [-1.5825, -2.2386],\n",
      "        [-2.3290, -2.2910],\n",
      "        [-2.2402, -2.2346]]) tensor([ 0.4949, -0.1472,  0.2724, -1.0188, -0.4604,  0.9419,  0.9121, -0.1346])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "tensor_dataloader = DataLoader(t_dataset,   # 封装的对象\n",
    "                               batch_size=8,     # 输出的batch size\n",
    "                               shuffle=True,     # 随机输出\n",
    "                               num_workers=0)    # 只有1个进程\n",
    "\n",
    "# 以for循环形式输出\n",
    "for data, target in tensor_dataloader:\n",
    "    print(data, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transforms\n",
    "实现对数据集的预处理、数据增强等一系列操作，包括compose，to tensor等等操作，或者自定义操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 看看别人是怎用的\n",
    "# from PIL import Image\n",
    "# from torchvision import transforms\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# class MyDataset(Dataset):\n",
    "#     def __init__(self, data_dir, transforms=None):\n",
    "#         self.data_info = self.get_img_info(data_dir)\n",
    "#         self.transforms = transforms\n",
    "\n",
    "#     def __getitem__(self, item):\n",
    "#         path_img, label = self.data_info[item]\n",
    "#         image = Image.open(path_img).convert('RGB')\n",
    "#         # 使用定义好的transforms，对数据进行处理\n",
    "#         if self.transforms is not None:\n",
    "#             image = self.transforms(image)\n",
    "\n",
    "#         return image, label\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data_info)\n",
    "\n",
    "# train_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "#                                        transforms.RandomHorizontalFlip(0.5)])\n",
    "# train_dataset = MyDataset(data_dir, train_transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mynet(\n",
      "  (fc1): Linear(in_features=2, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 定义模型\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Mynet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1=nn.Linear(2,256)\n",
    "        self.fc2=nn.Linear(256,128)\n",
    "        self.fc3=nn.Linear(128,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y1=F.relu(self.fc1(x))\n",
    "        y2=F.relu(self.fc2(y1))\n",
    "        return self.fc3(y2).squeeze(-1)\n",
    "\n",
    "model=Mynet()\n",
    "print(model)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader,model,loss_fn,optimizer):\n",
    "    size=len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch,(X,y) in enumerate(dataloader):\n",
    "        y_hat=model(X)\n",
    "        loss=loss_fn(y_hat,y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch%2==0:\n",
    "            loss,current=loss.item(),batch*len(X)\n",
    "            print('current{} loss is {}'.format(current,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "---------------------\n",
      "current0 loss is 0.7304892539978027\n",
      "current16 loss is 0.7407615780830383\n",
      "current32 loss is 0.5027478337287903\n",
      "current48 loss is 0.9335276484489441\n",
      "current64 loss is 1.7129956483840942\n",
      "current80 loss is 0.40431901812553406\n",
      "current96 loss is 0.5802524089813232\n",
      "current112 loss is 0.6539322733879089\n",
      "1\n",
      "---------------------\n",
      "current0 loss is 0.5178716778755188\n",
      "current16 loss is 0.4191153645515442\n",
      "current32 loss is 0.8373433351516724\n",
      "current48 loss is 0.6799297332763672\n",
      "current64 loss is 0.34582436084747314\n",
      "current80 loss is 1.3609826564788818\n",
      "current96 loss is 0.9500986933708191\n",
      "current112 loss is 1.084706425666809\n",
      "2\n",
      "---------------------\n",
      "current0 loss is 0.5745450258255005\n",
      "current16 loss is 1.5159958600997925\n",
      "current32 loss is 0.44541501998901367\n",
      "current48 loss is 1.9395828247070312\n",
      "current64 loss is 1.1680681705474854\n",
      "current80 loss is 0.22981217503547668\n",
      "current96 loss is 0.18289926648139954\n",
      "current112 loss is 0.5761668682098389\n",
      "3\n",
      "---------------------\n",
      "current0 loss is 1.0842069387435913\n",
      "current16 loss is 1.3421406745910645\n",
      "current32 loss is 0.23269230127334595\n",
      "current48 loss is 0.35868826508522034\n",
      "current64 loss is 1.130373239517212\n",
      "current80 loss is 1.3055367469787598\n",
      "current96 loss is 0.9277912378311157\n",
      "current112 loss is 0.5553120374679565\n",
      "4\n",
      "---------------------\n",
      "current0 loss is 0.5415144562721252\n",
      "current16 loss is 0.8363086581230164\n",
      "current32 loss is 0.15951932966709137\n",
      "current48 loss is 1.2515003681182861\n",
      "current64 loss is 0.7936110496520996\n",
      "current80 loss is 0.9170923829078674\n",
      "current96 loss is 1.0341882705688477\n",
      "current112 loss is 0.7724193930625916\n"
     ]
    }
   ],
   "source": [
    "epoch=5\n",
    "for t in range(epoch):\n",
    "    print(str(t)+'\\n---------------------')\n",
    "    train(tensor_dataloader,model,loss_fn,optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('datastudy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3273f24b59296cb560c65bed09ded30a701d6324543f77e854781b9b57ede183"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
