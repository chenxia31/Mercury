{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0x01 读取数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在读取数据集的阶段，是将包含原始的text的raw dataset，通过每一行读取的方式，并通过一定间隔（char=1、char=2或者是string）来得到token，在token的基础上来获取值得分析的vocab，在vocab中索引index的基础上来对原始的raw dataset进行重新编号得到corpus，在之后训练的过程还需要对数据进行one-hot和reshape处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "## 李沐中d2l服务器中存储的数据\n",
    "DATA_HUB = dict()\n",
    "DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'\n",
    "dataset='time_machine'\n",
    "dataset_sha256='090b5e7e70c295757f55df93cb0a180b9691891a'\n",
    "DATA_HUB[dataset]=(DATA_URL+'timemachine.txt',dataset_sha256)\n",
    "\n",
    "import os\n",
    "import hashlib\n",
    "import requests\n",
    "import re\n",
    "\n",
    "def download(name, cache_dir=os.path.join('..', 'data')):  #@save\n",
    "    \"\"\"下载一个DATA_HUB中的文件 返回本地文件名\"\"\"\n",
    "    url, sha1_hash = DATA_HUB[name]\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    fname = os.path.join(cache_dir, url.split('/')[-1])\n",
    "    if os.path.exists(fname):\n",
    "        sha1 = hashlib.sha1()\n",
    "        with open(fname, 'rb') as f:\n",
    "            while True:\n",
    "                data = f.read(1048576)\n",
    "                if not data:\n",
    "                    break\n",
    "                sha1.update(data)\n",
    "        if sha1.hexdigest() == sha1_hash:\n",
    "            return fname  # 命中缓存\n",
    "    r = requests.get(url, stream=True, verify=True)\n",
    "    with open(fname, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "    return fname\n",
    "\n",
    "\n",
    "def read_time_machine():  #@save\n",
    "    \"\"\"将时间机器数据集加载到文本行的列表中\"\"\"\n",
    "    with open(download('time_machine'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "\n",
    "def tokenize(lines, token='word'):  #@save\n",
    "    \"\"\"将文本行拆分为单词或字符\"\"\"\n",
    "    if token == 'word':\n",
    "        return [line.split(' ') for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('错误：未知令牌类型：'+token)\n",
    "\n",
    "import collections  \n",
    "class Vocab:  #@save\n",
    "    \"\"\"文本词表\"\"\"\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        # 按出现频率排序\n",
    "        counter = count_corpus(tokens)\n",
    "        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n",
    "                                   reverse=True)\n",
    "        # 未知词元的索引为0\n",
    "        self.idx_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_idx = {token: idx\n",
    "                             for idx, token in enumerate(self.idx_to_token)}\n",
    "        for token, freq in self._token_freqs:\n",
    "            if freq < min_freq:\n",
    "                break\n",
    "            if token not in self.token_to_idx:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "    @property\n",
    "    def unk(self):  # 未知词元的索引为0\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self._token_freqs\n",
    "\n",
    "def count_corpus(tokens):  #@save\n",
    "    \"\"\"统计词元的频率\"\"\"\n",
    "    # 这里的tokens是1D列表或2D列表\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        # 将词元列表展平成一个列表\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)\n",
    "\n",
    "def load_corpus_time_machine(max_tokens=-1):  #@save\n",
    "    \"\"\"返回时光机器数据集的词元索引列表和词表\"\"\"\n",
    "    lines = read_time_machine()\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    vocab = Vocab(tokens)\n",
    "    # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，\n",
    "    # 所以将所有文本行展平到一个列表中\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocab\n",
    "\n",
    "def seq_data_iter_random(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"使用随机抽样生成一个小批量子序列\"\"\"\n",
    "    # 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1\n",
    "    corpus = corpus[random.randint(0, num_steps - 1):]\n",
    "    # 减去1，是因为我们需要考虑标签\n",
    "    num_subseqs = (len(corpus) - 1) // num_steps\n",
    "    # 长度为num_steps的子序列的起始索引\n",
    "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
    "    # 在随机抽样的迭代过程中，\n",
    "    # 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻\n",
    "    random.shuffle(initial_indices)\n",
    "\n",
    "    def data(pos):\n",
    "        # 返回从pos位置开始的长度为num_steps的序列\n",
    "        return corpus[pos: pos + num_steps]\n",
    "\n",
    "    num_batches = num_subseqs // batch_size\n",
    "    for i in range(0, batch_size * num_batches, batch_size):\n",
    "        # 在这里，initial_indices包含子序列的随机起始索引\n",
    "        initial_indices_per_batch = initial_indices[i: i + batch_size]\n",
    "        X = [data(j) for j in initial_indices_per_batch]\n",
    "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
    "        yield torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"使用顺序分区生成一个小批量子序列\"\"\"\n",
    "    # 从随机偏移量开始划分序列\n",
    "    offset = random.randint(0, num_steps)\n",
    "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
    "    Xs = torch.tensor(corpus[offset: offset + num_tokens])\n",
    "    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
    "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
    "    num_batches = Xs.shape[1] // num_steps\n",
    "    for i in range(0, num_steps * num_batches, num_steps):\n",
    "        X = Xs[:, i: i + num_steps]\n",
    "        Y = Ys[:, i: i + num_steps]\n",
    "        yield X, Y\n",
    "\n",
    "class SeqDataLoader:  #@save\n",
    "    \"\"\"加载序列数据的迭代器\"\"\"\n",
    "    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
    "        if use_random_iter:\n",
    "            self.data_iter_fn = seq_data_iter_random\n",
    "        else:\n",
    "            self.data_iter_fn = seq_data_iter_sequential\n",
    "        self.corpus, self.vocab = load_corpus_time_machine(max_tokens)\n",
    "        self.batch_size, self.num_steps = batch_size, num_steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)\n",
    "\n",
    "def load_data_time_machine(batch_size, num_steps,  #@save\n",
    "                           use_random_iter=False, max_tokens=10000):\n",
    "    \"\"\"返回时光机器数据集的迭代器和词表\"\"\"\n",
    "    data_iter = SeqDataLoader(\n",
    "        batch_size, num_steps, use_random_iter, max_tokens)\n",
    "    return data_iter, data_iter.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0x02 参数初始化、模型定义和优化器选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size,num_step=32,35\n",
    "train_iter,vocab=load_data_time_machine(batch_size,num_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将每个词元变换成更具有表现力的特征向量\n",
    "X = torch.arange(10).reshape((2, 5))\n",
    "F.one_hot(X.T, 28).shape\n",
    "# 每次采样的小批量数据形状是二维张量，one——hot将小批量数据转换称成为三维张量\n",
    "# (batch size，num_steps,vocab_size）->(num_steps,batch_size,vocab_size)\n",
    "\n",
    "# 初始化模型参数\n",
    "def get_params(vocab_size,num_hiddens,device):\n",
    "    num_inputs=num_outputs=vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape,device=device)*0.01\n",
    "    \n",
    "    # hidden layer\n",
    "    W_xh=normal((num_inputs,num_hiddens))\n",
    "    W_hh=normal((num_hiddens,num_hiddens))\n",
    "    b_h=torch.zeros(num_hiddens,device=device)\n",
    "\n",
    "    # output\n",
    "    W_hq=normal((num_hiddens,num_outputs))\n",
    "    b_q=torch.zeros(num_outputs,device=device)\n",
    "\n",
    "    # append gradient\n",
    "    params=[W_xh,W_hh,b_h,W_hq,b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义hidden state的初始值\n",
    "def init_rnn_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), )\n",
    "\n",
    "def rnn(inputs,state,params):\n",
    "    W_xh,W_hh,b_h,W_hq,b_q=params\n",
    "    H,=state\n",
    "    outputs=[]\n",
    "    # X的形状：（batch_size，vacab_size）\n",
    "    for X in inputs:\n",
    "        H=torch.tanh(torch.mm(X,W_xh)+torch.mm(H,W_hh)+b_h)\n",
    "        Y=torch.mm(H,W_hq)+b_q\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs,dim=0),(H,)\n",
    "\n",
    "class RNNModelScratch: #@save\n",
    "    \"\"\"从零开始实现的循环神经网络模型\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, device,\n",
    "                 get_params, init_state, forward_fn):\n",
    "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
    "        self.params = get_params(vocab_size, num_hiddens, device)\n",
    "        self.init_state, self.forward_fn = init_state, forward_fn\n",
    "\n",
    "    def __call__(self, X, state):\n",
    "        # 转换为one-hot变量\n",
    "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
    "        return self.forward_fn(X, state, self.params)\n",
    "\n",
    "    def begin_state(self, batch_size, device):\n",
    "        return self.init_state(batch_size, self.num_hiddens, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0x03 定义RNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 28]), 1, torch.Size([2, 512]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_hiddens = 512\n",
    "def try_gpu(i=0):  #@save\n",
    "    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()\"\"\"\n",
    "    if torch.cuda.device_count() >= i + 1:\n",
    "        return torch.device(f'cuda:{i}')\n",
    "    return torch.device('cpu')\n",
    "net = RNNModelScratch(len(vocab), num_hiddens, try_gpu(), get_params,\n",
    "                      init_rnn_state, rnn)\n",
    "state = net.begin_state(X.shape[0], try_gpu())\n",
    "Y, new_state = net(X.to(try_gpu()), state)\n",
    "Y.shape, len(new_state), new_state[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 0x04 预测\n",
    "def predict_ch8(prefix, num_preds, net, vocab, device):  #@save\n",
    "    \"\"\"在prefix后面生成新字符\"\"\"\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
    "    for y in prefix[1:]:  # 预热期\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):  # 预测num_preds步\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time traveller mrosnsnsns'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ch8('time traveller ', 10, net, vocab, try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta):  #@save\n",
    "    \"\"\"裁剪梯度\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0x05 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n",
    "    \"\"\"训练网络一个迭代周期（定义见第8章）\"\"\"\n",
    "    state = None\n",
    "    for X, Y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            # 在第一次迭代或使用随机抽样时初始化state\n",
    "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "        else:\n",
    "            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
    "                # state对于nn.GRU是个 tensor\n",
    "                state.detach_()\n",
    "            else:\n",
    "                # 在RNN和LSTM中是标量 scholar\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        y = Y.T.reshape(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat, state = net(X, state)\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            # 因为已经调用了mean函数\n",
    "            updater(batch_size=1)\n",
    "    return math.exp(l * y.numel() / y.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):\n",
    "    \"\"\"Minibatch stochastic gradient descent.\n",
    "\n",
    "    Defined in :numref:`sec_utils`\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()\n",
    "\n",
    "def train_ch8(net, train_iter, vocab, lr, num_epochs, device,\n",
    "              use_random_iter=False):\n",
    "    \"\"\"训练模型（定义见第8章）\"\"\"\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 初始化\n",
    "    if isinstance(net, nn.Module):\n",
    "        updater = torch.optim.SGD(net.parameters(), lr)\n",
    "    else:\n",
    "        updater = lambda batch_size: sgd(params=net.params, lr=lr,batch_size=batch_size)\n",
    "    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)\n",
    "    # 训练和预测\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl = train_epoch_ch8(\n",
    "            net, train_iter, loss, updater, device, use_random_iter)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(predict('time traveller'))\n",
    "            print(ppl)\n",
    "    print(f'困惑度 {ppl:.1f}{str(device)}')\n",
    "    print(predict('time traveller'))\n",
    "    print(predict('traveller'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time traveller the the the the the the the the the the the the t\n",
      "12.529913291355326\n",
      "time traveller the thent the thent the thent the thent the thent\n",
      "10.223937931787818\n",
      "time travellere the the the the the the the the the the the the \n",
      "8.84681516274112\n",
      "time traveller the the the the the the the the the the the the t\n",
      "8.35704229719762\n",
      "time traveller and the the the the the the the the the the the t\n",
      "8.033088304390821\n",
      "time traveller and the the this the the this the the this the th\n",
      "7.856264546710224\n",
      "time traveller and the the this the the this the the this the th\n",
      "7.575421697403906\n",
      "time traveller and the the the the the the the the the the the t\n",
      "7.556419887393418\n",
      "time traveller and the the the the the the the the the the the t\n",
      "7.312872729284338\n",
      "time traveller and the the the the the the the the the the the t\n",
      "7.059141270177114\n",
      "time traveller aid the the the the the the the the the the the t\n",
      "6.98544169246624\n",
      "time traveller there which ally there whan there and the the tha\n",
      "6.760479578221003\n",
      "time traveller s ance freat in the that in the that in the that \n",
      "6.594609167475016\n",
      "time traveller at in the thatheres the thathere wather the the t\n",
      "6.302718237561713\n",
      "time traveller the madical man the and and the and are mad have \n",
      "5.50733934363947\n",
      "time traveller phed and the time travellerimendif spacentimens a\n",
      "6.127430491553994\n",
      "time traveller s the time traveller s the thing the wight ofis t\n",
      "4.800961378264821\n",
      "time traveller but that is meat this geat four that uply hadile \n",
      "4.330163317374896\n",
      "time traveller butwe that upathe simectian whing is a andis y do\n",
      "3.553754519857368\n",
      "time travellerimed a toun thesperifconnsthe thing sise sithe mur\n",
      "2.860967171433136\n",
      "time traveller three of the three dimensions of space and to the\n",
      "2.6596024410857235\n",
      "time travellerimettat oplerits on ally three dimensions at y ack\n",
      "2.3489189693034045\n",
      "time travellerips cans on the time traveller peac hard hat ge nl\n",
      "1.9956155306961496\n",
      "time traveller priceidet aboug th arelor but some fill speou whi\n",
      "2.6138776310552996\n",
      "time travellerit s atainst reason said the very ylurgect hriag b\n",
      "1.4465432940790026\n",
      "time travellericknd way to neasongbeg baig the pion of some talk\n",
      "1.543944318157482\n",
      "time travellerit soaghitnorism thiphthis ment whith rad oners wa\n",
      "1.4354303633799235\n",
      "time traveller thice helh a dimensions of spaceeg his featt mole\n",
      "1.6230321274696713\n",
      "time traveller held in a four ho kno the tome thai buun theingth\n",
      "1.2143100048537276\n",
      "time traveller thi gery coras in thing thee ptre werecabe then t\n",
      "1.532649488847354\n",
      "time traveller held in his hand was a glint any thill sach to so\n",
      "1.7356684532392888\n",
      "time traveller smid in fist allight of mand but a caplonist asow\n",
      "1.3352241185910805\n",
      "time traveller proceeded anyreal body mustre tlle to siast ano t\n",
      "1.2208897297699604\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "1.0184134171311514\n",
      "time traveller ffrenewinly pave in the figbut son and the kedm r\n",
      "1.0394641554639312\n",
      "time traveller smient for think do farm lhrassot candinee iont l\n",
      "1.032718825432529\n",
      "time traveller proceeded anyreal body must have expensive tattee\n",
      "1.1619427359673336\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "1.0259994981408467\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "1.0191323123534157\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "1.0305009888493317\n",
      "time traveller of cimanesy rovesaide travel man go began of spow\n",
      "1.0095844304096881\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "1.0101130242796241\n",
      "time traveller fither have atserareits they rould te mel car ge \n",
      "1.0056873907183221\n",
      "time travelleryou can show black is white by argument said filby\n",
      "1.0054670784247346\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "1.0221636201200985\n",
      "time traveller but now you begin to seethe object of my investig\n",
      "1.0038131298908914\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "1.0078576497849554\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "1.0030160676097333\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "1.0182181323158384\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "1.0030152876102287\n",
      "困惑度 1.0cpu\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "traveller with a slight accession ofcheerfulness really thi\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 500, 1\n",
    "train_ch8(net, train_iter, vocab, lr, num_epochs, try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ppl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mppl\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ppl' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
