{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GYM-CartPole例子\n",
    "通过控制一辆小车让杆立起来，首先初始化环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "# 导入gym的python接口环境包\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#env for environment，用来构建实验环境\n",
    "env=gym.make('CartPole-v1')\n",
    "\n",
    "#重置一个回合\n",
    "env.reset()\n",
    "\n",
    "for _ in range(1000):\n",
    "    # 显示图形界面\n",
    "    # env.render()\n",
    "    env.render()\n",
    "    # 从动作空间中随机选取一个动作\n",
    "    action=env.action_space.sample()\n",
    "    # 提交动作，并反馈对应的参数 observation、reward、done、info\n",
    "    result=env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['CartPole-v0', 'CartPole-v1', 'MountainCar-v0', 'MountainCarContinuous-v0', 'Pendulum-v1', 'Acrobot-v1', 'LunarLander-v2', 'LunarLanderContinuous-v2', 'BipedalWalker-v3', 'BipedalWalkerHardcore-v3', 'CarRacing-v2', 'Blackjack-v1', 'FrozenLake-v1', 'FrozenLake8x8-v1', 'CliffWalking-v0', 'Taxi-v3', 'Reacher-v2', 'Reacher-v4', 'Pusher-v2', 'Pusher-v4', 'InvertedPendulum-v2', 'InvertedPendulum-v4', 'InvertedDoublePendulum-v2', 'InvertedDoublePendulum-v4', 'HalfCheetah-v2', 'HalfCheetah-v3', 'HalfCheetah-v4', 'Hopper-v2', 'Hopper-v3', 'Hopper-v4', 'Swimmer-v2', 'Swimmer-v3', 'Swimmer-v4', 'Walker2d-v2', 'Walker2d-v3', 'Walker2d-v4', 'Ant-v2', 'Ant-v3', 'Ant-v4', 'Humanoid-v2', 'Humanoid-v3', 'Humanoid-v4', 'HumanoidStandup-v2', 'HumanoidStandup-v4'])\n"
     ]
    }
   ],
   "source": [
    "# 查看GYM中所有的注册的环境\n",
    "from gym import envs\n",
    "print(envs.registry.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 小车上山 MountainCar-V0\n",
    "观测如何与Gym库进行交互"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "观测空间=Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
      "动作空间=Discrete(3)\n",
      "观测范围=[-1.2  -0.07]~[0.6  0.07]\n",
      "动作数=3\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env=gym.make('MountainCar-v0')\n",
    "print('观测空间={}'.format(env.observation_space))\n",
    "print('动作空间={}'.format(env.action_space))\n",
    "print('观测范围={}~{}'.format(env.observation_space.low,env.observation_space.high))\n",
    "print('动作数={}'.format(env.action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义智能体\n",
    "在初始化任务的观测空间和动作空间之后，通常需要使用一个智能体来进行与环境的交互\n",
    "这样的智能体往往是需要我们自己实现的，包括learn和decision的方法\n",
    "decide（）方法实现决策功能\n",
    "learn（）方法实现学习功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BespokeAgent:\n",
    "    def __init__(self,env):\n",
    "        pass\n",
    "        \n",
    "    def decide(self,observation):\n",
    "        # 这里相当于是根据o，返回自己a的过程\n",
    "        position,velocity=observation[0],observation[1]\n",
    "        print(observation)\n",
    "        lb=min(-0.09*(position+0.25)**2+0.03,0.3*(position+0.9)**4-0.008)\n",
    "        ub=-0.07*(position+0.38)**2+0.07\n",
    "        if lb<velocity<ub:\n",
    "            action=2\n",
    "        else:\n",
    "            action=0\n",
    "        return action\n",
    "\n",
    "    def learn(self,*args):\n",
    "        pass\n",
    "\n",
    "agent=BespokeAgent(env)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agent与env交互"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_montecarlo(env,agent,render=False,train=False):\n",
    "    episode_reward=0\n",
    "    observation=env.reset()\n",
    "    observation=observation[0]\n",
    "    i=0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action=agent.decide(observation)\n",
    "        result=env.step(action)\n",
    "\n",
    "        episode_reward+=result[1]\n",
    "        if train:\n",
    "            agent.learn()\n",
    "        if result[2]:\n",
    "            break\n",
    "        observation=result[0]\n",
    "        i+=1\n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.46997347  0.        ]\n",
      "[-0.47137395 -0.00140046]\n",
      "[-0.4741645  -0.00279054]\n",
      "[-0.4783244  -0.00415994]\n",
      "[-0.48382288 -0.00549846]\n",
      "[-0.49061894 -0.00679607]\n",
      "[-0.49866197 -0.00804301]\n",
      "[-0.50789183 -0.00922987]\n",
      "[-0.51823944 -0.01034762]\n",
      "[-0.52962726 -0.01138782]\n",
      "[-0.5419699  -0.01234261]\n",
      "[-0.55517477 -0.01320489]\n",
      "[-0.5691432  -0.01396843]\n",
      "[-0.5837711  -0.01462791]\n",
      "[-0.59895015 -0.01517906]\n",
      "[-0.6145689  -0.01561873]\n",
      "[-0.6305138  -0.01594489]\n",
      "[-0.64667046 -0.0161567 ]\n",
      "[-0.66292495 -0.01625449]\n",
      "[-0.6791647  -0.01623971]\n",
      "[-0.69527954 -0.01611486]\n",
      "[-0.711163   -0.01588343]\n",
      "[-0.72671276 -0.01554977]\n",
      "[-0.7418317  -0.01511895]\n",
      "[-0.75642836 -0.01459665]\n",
      "[-0.77041733 -0.01398899]\n",
      "[-0.7837198  -0.01330243]\n",
      "[-0.79626334 -0.01254359]\n",
      "[-0.8079825  -0.01171915]\n",
      "[-0.8188183  -0.01083576]\n",
      "[-0.8287182  -0.00989993]\n",
      "[-0.8376362  -0.00891798]\n",
      "[-0.8455321  -0.00789597]\n",
      "[-0.85037184 -0.00483971]\n",
      "[-0.8521349  -0.00176302]\n",
      "[-0.85081387  0.001321  ]\n",
      "[-0.8464144   0.00439953]\n",
      "[-0.8389548   0.00745955]\n",
      "[-0.8284675   0.01048735]\n",
      "[-0.8149993   0.01346816]\n",
      "[-0.79861355  0.01638573]\n",
      "[-0.7793914   0.01922218]\n",
      "[-0.7574336   0.02195781]\n",
      "[-0.73286235  0.02457123]\n",
      "[-0.70582277  0.02703962]\n",
      "[-0.6764835   0.02933924]\n",
      "[-0.6450374   0.03144609]\n",
      "[-0.61170053  0.03333687]\n",
      "[-0.5767106   0.03498996]\n",
      "[-0.540324    0.03638662]\n",
      "[-0.50281197  0.037512  ]\n",
      "[-0.46445575  0.03835621]\n",
      "[-0.4255408   0.03891496]\n",
      "[-0.38635072  0.03919008]\n",
      "[-0.34716114  0.03918956]\n",
      "[-0.30823395  0.03892721]\n",
      "[-0.2698119   0.03842203]\n",
      "[-0.23211464  0.03769726]\n",
      "[-0.19533536  0.03677928]\n",
      "[-0.15963897  0.03569639]\n",
      "[-0.12516132  0.03447765]\n",
      "[-0.09200948  0.03315183]\n",
      "[-0.06026302  0.03174647]\n",
      "[-0.02997581  0.03028721]\n",
      "[-0.0011785   0.02879731]\n",
      "[0.02611883 0.02729733]\n",
      "[0.05192383 0.025805  ]\n",
      "[0.07625909 0.02433527]\n",
      "[0.0991595  0.02290041]\n",
      "[0.12066971 0.02151021]\n",
      "[0.14084196 0.02017224]\n",
      "[0.15973406 0.0188921 ]\n",
      "[0.17740776 0.0176737 ]\n",
      "[0.19392724 0.01651949]\n",
      "[0.20935802 0.01543078]\n",
      "[0.2237659  0.01440788]\n",
      "[0.23721623 0.01345034]\n",
      "[0.24977337 0.01255712]\n",
      "[0.26150012 0.01172674]\n",
      "[0.2724575  0.01095739]\n",
      "[0.28270453 0.01024704]\n",
      "[0.29229808 0.00959354]\n",
      "[0.30129278 0.00899469]\n",
      "[0.30974105 0.00844827]\n",
      "[0.31769317 0.00795213]\n",
      "[0.32519737 0.00750419]\n",
      "[0.33229986 0.0071025 ]\n",
      "[0.33904508 0.00674522]\n",
      "[0.3454758  0.00643071]\n",
      "[0.35163328 0.00615747]\n",
      "[0.35755745 0.00592418]\n",
      "[0.36328718 0.00572973]\n",
      "[0.3688604  0.00557321]\n",
      "[0.3743143  0.00545391]\n",
      "[0.37968564 0.00537133]\n",
      "[0.38501084 0.0053252 ]\n",
      "[0.39032632 0.00531548]\n",
      "[0.3956687  0.00534236]\n",
      "[0.40107495 0.00540626]\n",
      "[0.40658283 0.00550789]\n",
      "[0.41223103 0.00564818]\n",
      "[0.41805938 0.00582838]\n",
      "[0.42410937 0.00604999]\n",
      "[0.43042424 0.00631485]\n",
      "[0.43704933 0.0066251 ]\n",
      "[0.44403258 0.00698325]\n",
      "[0.45142475 0.00739216]\n",
      "[0.4592798  0.00785507]\n",
      "[0.46765548 0.00837567]\n",
      "[0.47661355 0.00895806]\n",
      "[0.4862204  0.00960684]\n",
      "[0.49654746 0.01032709]\n",
      "回合奖励=-112.0\n"
     ]
    }
   ],
   "source": [
    "episode_reward=play_montecarlo(env,agent,render=False)\n",
    "print('回合奖励={}'.format(episode_reward))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('datastudy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3273f24b59296cb560c65bed09ded30a701d6324543f77e854781b9b57ede183"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
