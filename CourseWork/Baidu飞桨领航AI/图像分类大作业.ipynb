{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 任务描述\n",
    "\n",
    "如何根据据图像的视觉内容为图像赋予一个语义类别是图像分类的目标，也是图像检索、图像内容分析和目标识别等问题的基础。\n",
    "本实践旨在通过一个美食分类的案列，让大家理解和掌握如何使用飞桨2.0搭建一个卷积神经网络。\n",
    "特别提示：本实践所用数据集均来自互联网，请勿用于商务用途。\n",
    "\n",
    "解压文件，使用train.csv训练，测试使用val.csv。最后以在val上的准确率作为最终分数。\n",
    "\n",
    "## ✓调优\n",
    "思考并动手进行调优，以在验证集上的准确率为评价指标，验证集上准确率越高，得分越高！模型大家可以更换，调参技巧任选，代码需要大家自己调通。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!unzip -oq /home/aistudio/data/data120156/lemon_homework.zip\r\n",
    "!unzip -oq /home/aistudio/lemon_homework/lemon_lesson.zip\r\n",
    "!unzip -oq /home/aistudio/lemon_lesson/test_images.zip\r\n",
    "!unzip -oq /home/aistudio/lemon_lesson/train_images.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 导入所需要的库\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "from paddle.io import Dataset\n",
    "import paddle.vision.transforms as T\n",
    "import paddle.nn.functional as F\n",
    "from paddle.metric import Accuracy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df=pd.read_csv('lemon_lesson/train_images.csv')\n",
    "d=df['class_num'].hist().get_figure()\n",
    "# 图像分类竞赛常见难点\n",
    "# 类别不均衡\n",
    "# one-shot和few-shot分类\n",
    "# 细粒度分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "图像标准化与归一化，最常见的图像预处理方式有两种，一种是图标标准化处理，将数据按照比例缩放，使之落入一个特定的区间中，将数据通过去均值，实现中心化。第二种是数据归一化，将数据统一映射到0-1区间中\n",
    "它的作用\n",
    "1. 有利于初始化的进行\n",
    "2. 避免给梯度数值更新带来数值问题\n",
    "3. 有利于学习率数值的调整\n",
    "4. 加快寻找最优解速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义数据预处理\n",
    "\n",
    "data_transforms = T.Compose([\n",
    "    T.Resize(size=(224, 224)),\n",
    "    T.RandomHorizontalFlip(1),\n",
    "    T.RandomVerticalFlip(1),\n",
    "    T.Transpose(),    # HWC -> CHW\n",
    "    T.Normalize(\n",
    "        mean=[0, 0, 0],        # 归一化\n",
    "        std=[255, 255, 255],\n",
    "        to_rgb=True)    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1102\n",
      "881\n",
      "221\n"
     ]
    }
   ],
   "source": [
    "## 数据集划分\n",
    "\n",
    "train_images = pd.read_csv('lemon_lesson/train_images.csv', usecols=['id','class_num'])\n",
    "\n",
    "# 划分训练集和校验集\n",
    "all_size = len(train_images)\n",
    "print(all_size)\n",
    "train_size = int(all_size * 0.8)\n",
    "train_image_path_list = train_images[:train_size]\n",
    "val_image_path_list = train_images[train_size:]\n",
    "\n",
    "print(len(train_image_path_list))\n",
    "print(len(val_image_path_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 构建Dataset\n",
    "class MyDataset(paddle.io.Dataset):\n",
    "    \"\"\"\n",
    "    步骤一：继承paddle.io.Dataset类\n",
    "    \"\"\"\n",
    "    def __init__(self, train_list, val_list, mode='train'):\n",
    "        \"\"\"\n",
    "        步骤二：实现构造函数，定义数据读取方式\n",
    "        \"\"\"\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.data = []\n",
    "        # 借助pandas读取csv文件\n",
    "        self.train_images = train_list\n",
    "        self.test_images = val_list\n",
    "        if mode == 'train':\n",
    "            # 读train_images.csv中的数据\n",
    "            for row in self.train_images.itertuples():\n",
    "                self.data.append(['train_images/'+getattr(row, 'id'), getattr(row, 'class_num')])\n",
    "        else:\n",
    "            # 读test_images.csv中的数据\n",
    "            for row in self.test_images.itertuples():\n",
    "                self.data.append(['train_images/'+getattr(row, 'id'), getattr(row, 'class_num')])\n",
    "\n",
    "    def load_img(self, image_path):\n",
    "        # 实际使用时使用Pillow相关库进行图片读取即可，这里我们对数据先做个模拟\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        步骤三：实现__getitem__方法，定义指定index时如何获取数据，并返回单条数据（训练数据，对应的标签）\n",
    "        \"\"\"\n",
    "        image = self.load_img(self.data[index][0])\n",
    "        label = self.data[index][1]\n",
    "\n",
    "        return data_transforms(image), np.array(label, dtype='int64')\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        步骤四：实现__len__方法，返回数据集总数目\n",
    "        \"\"\"\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 定义数据loader\n",
    "#train_loader\n",
    "train_dataset = MyDataset(train_list=train_image_path_list, val_list=val_image_path_list, mode='train')\n",
    "train_loader = paddle.io.DataLoader(train_dataset, places=paddle.CPUPlace(), batch_size=128, shuffle=True, num_workers=0)\n",
    "\n",
    "#val_loader\n",
    "val_dataset =MyDataset(train_list=train_image_path_list, val_list=val_image_path_list, mode='test')\n",
    "val_loader = paddle.io.DataLoader(val_dataset, places=paddle.CPUPlace(), batch_size=128, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============train dataset=============\n",
      "image shape: (3, 224, 224), label: 0\n",
      "Tensor(shape=[128, 3, 224, 224], dtype=float32, place=CPUPlace, stop_gradient=True,\n",
      "       [[[[0.10980392, 0.10196079, 0.10588235, ..., 0.10588235, 0.11372549, 0.14117648],\n",
      "          [0.11372549, 0.10980392, 0.10196079, ..., 0.11372549, 0.14509805, 0.16470589],\n",
      "          [0.14901961, 0.11764706, 0.10196079, ..., 0.15686275, 0.23137255, 0.25098041],\n",
      "          ...,\n",
      "          [0.50588238, 0.50588238, 0.50588238, ..., 0.85882354, 0.85882354, 0.85882354],\n",
      "          [0.50196081, 0.50196081, 0.50196081, ..., 0.85490197, 0.85490197, 0.85490197],\n",
      "          [0.49803922, 0.49803922, 0.49803922, ..., 0.85098040, 0.85098040, 0.85098040]],\n",
      "\n",
      "         [[0.06666667, 0.05882353, 0.06274510, ..., 0.08627451, 0.10196079, 0.13333334],\n",
      "          [0.07450981, 0.07058824, 0.06274510, ..., 0.09803922, 0.13725491, 0.15686275],\n",
      "          [0.11764706, 0.08627451, 0.07058824, ..., 0.14901961, 0.22745098, 0.25098041],\n",
      "          ...,\n",
      "          [0.43921569, 0.43921569, 0.43921569, ..., 0.72549021, 0.72549021, 0.72549021],\n",
      "          [0.43529412, 0.43529412, 0.43529412, ..., 0.72156864, 0.72156864, 0.72156864],\n",
      "          [0.43137255, 0.43137255, 0.43137255, ..., 0.71764708, 0.71764708, 0.71764708]],\n",
      "\n",
      "         [[0.08235294, 0.07450981, 0.07843138, ..., 0.08235294, 0.10196079, 0.13725491],\n",
      "          [0.08627451, 0.08627451, 0.07843138, ..., 0.09803922, 0.14117648, 0.16470589],\n",
      "          [0.12941177, 0.09411765, 0.08235294, ..., 0.15294118, 0.23529412, 0.26274511],\n",
      "          ...,\n",
      "          [0.51372552, 0.51372552, 0.51372552, ..., 0.78823531, 0.78823531, 0.78823531],\n",
      "          [0.50980395, 0.50980395, 0.50980395, ..., 0.78431374, 0.78431374, 0.78431374],\n",
      "          [0.50588238, 0.50588238, 0.50588238, ..., 0.78039217, 0.78039217, 0.78039217]]],\n",
      "\n",
      "\n",
      "        [[[0.09411765, 0.08627451, 0.09019608, ..., 0.10588235, 0.10980392, 0.14901961],\n",
      "          [0.09803922, 0.08627451, 0.08627451, ..., 0.11764706, 0.14509805, 0.16078432],\n",
      "          [0.13333334, 0.09803922, 0.09019608, ..., 0.16078432, 0.22745098, 0.26666668],\n",
      "          ...,\n",
      "          [0.45490196, 0.45490196, 0.45490196, ..., 0.75294119, 0.75294119, 0.75686276],\n",
      "          [0.45490196, 0.45490196, 0.45490196, ..., 0.74901962, 0.74901962, 0.74901962],\n",
      "          [0.46274510, 0.46274510, 0.46274510, ..., 0.74509805, 0.74509805, 0.74509805]],\n",
      "\n",
      "         [[0.07843138, 0.07058824, 0.07450981, ..., 0.07450981, 0.09019608, 0.13333334],\n",
      "          [0.08235294, 0.07058824, 0.07058824, ..., 0.09411765, 0.12941177, 0.14509805],\n",
      "          [0.11764706, 0.08235294, 0.07450981, ..., 0.14117648, 0.21568628, 0.25882354],\n",
      "          ...,\n",
      "          [0.43137255, 0.43137255, 0.43137255, ..., 0.65490198, 0.65490198, 0.65882355],\n",
      "          [0.42745098, 0.42745098, 0.42745098, ..., 0.65098041, 0.65098041, 0.65098041],\n",
      "          [0.42352942, 0.42352942, 0.42352942, ..., 0.64705884, 0.64705884, 0.64705884]],\n",
      "\n",
      "         [[0.06666667, 0.05882353, 0.06274510, ..., 0.07450981, 0.09019608, 0.13725491],\n",
      "          [0.07058824, 0.05882353, 0.05882353, ..., 0.09411765, 0.13333334, 0.15686275],\n",
      "          [0.10588235, 0.07058824, 0.06274510, ..., 0.14901961, 0.22745098, 0.27450982],\n",
      "          ...,\n",
      "          [0.45882353, 0.45882353, 0.45882353, ..., 0.66666669, 0.66666669, 0.67058825],\n",
      "          [0.45882353, 0.45882353, 0.45882353, ..., 0.66274512, 0.66274512, 0.66274512],\n",
      "          [0.45882353, 0.45882353, 0.45882353, ..., 0.65882355, 0.65882355, 0.65882355]]],\n",
      "\n",
      "\n",
      "        [[[0.14509805, 0.15686275, 0.19607843, ..., 0.13725491, 0.12156863, 0.16078432],\n",
      "          [0.10980392, 0.14117648, 0.13725491, ..., 0.28235295, 0.14117648, 0.14509805],\n",
      "          [0.08627451, 0.08627451, 0.12549020, ..., 0.30980393, 0.19215687, 0.16862746],\n",
      "          ...,\n",
      "          [0.19607843, 0.19607843, 0.19607843, ..., 0.42352942, 0.49019608, 0.54901963],\n",
      "          [0.19607843, 0.19607843, 0.19607843, ..., 0.41960785, 0.49019608, 0.54901963],\n",
      "          [0.19607843, 0.19607843, 0.19607843, ..., 0.41568628, 0.49411765, 0.55686277]],\n",
      "\n",
      "         [[0.14117648, 0.15294118, 0.19215687, ..., 0.14117648, 0.12941177, 0.16470589],\n",
      "          [0.10588235, 0.13725491, 0.13333334, ..., 0.28235295, 0.13725491, 0.14509805],\n",
      "          [0.08235294, 0.08235294, 0.12156863, ..., 0.30196080, 0.18431373, 0.16470589],\n",
      "          ...,\n",
      "          [0.07843138, 0.07843138, 0.07843138, ..., 0.23921569, 0.29411766, 0.34901962],\n",
      "          [0.07843138, 0.07843138, 0.07843138, ..., 0.23921569, 0.29803923, 0.35294119],\n",
      "          [0.07843138, 0.07843138, 0.07843138, ..., 0.23529412, 0.30196080, 0.36470589]],\n",
      "\n",
      "         [[0.17254902, 0.18431373, 0.22352941, ..., 0.18823530, 0.17647059, 0.21176471],\n",
      "          [0.12941177, 0.16078432, 0.15686275, ..., 0.32941177, 0.18431373, 0.18823530],\n",
      "          [0.10196079, 0.10196079, 0.14509805, ..., 0.34117648, 0.22352941, 0.20392157],\n",
      "          ...,\n",
      "          [0.07058824, 0.07058824, 0.07058824, ..., 0.09019608, 0.09411765, 0.10980392],\n",
      "          [0.07058824, 0.07058824, 0.07058824, ..., 0.08627451, 0.08627451, 0.10196079],\n",
      "          [0.07058824, 0.07058824, 0.07058824, ..., 0.08235294, 0.09019608, 0.10588235]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.12156863, 0.10588235, 0.15686275, ..., 0.10980392, 0.10196079, 0.17647059],\n",
      "          [0.10588235, 0.15294118, 0.14509805, ..., 0.19215687, 0.10980392, 0.13725491],\n",
      "          [0.08235294, 0.09019608, 0.12156863, ..., 0.27450982, 0.14117648, 0.15686275],\n",
      "          ...,\n",
      "          [0.16862746, 0.17254902, 0.16470589, ..., 0.26274511, 0.26666668, 0.27058825],\n",
      "          [0.16470589, 0.16862746, 0.16078432, ..., 0.25882354, 0.26274511, 0.26274511],\n",
      "          [0.16078432, 0.16470589, 0.16078432, ..., 0.25098041, 0.25490198, 0.25882354]],\n",
      "\n",
      "         [[0.09803922, 0.08235294, 0.13333334, ..., 0.11372549, 0.11372549, 0.19215687],\n",
      "          [0.08627451, 0.12941177, 0.12549020, ..., 0.18823530, 0.10980392, 0.14117648],\n",
      "          [0.06274510, 0.07450981, 0.10588235, ..., 0.26666668, 0.13725491, 0.15686275],\n",
      "          ...,\n",
      "          [0.05098039, 0.05490196, 0.04705882, ..., 0.07843138, 0.07450981, 0.07450981],\n",
      "          [0.04705882, 0.05098039, 0.04313726, ..., 0.07843138, 0.07450981, 0.07450981],\n",
      "          [0.04313726, 0.04705882, 0.04313726, ..., 0.07450981, 0.07058824, 0.07058824]],\n",
      "\n",
      "         [[0.14901961, 0.13333334, 0.18431373, ..., 0.15686275, 0.12941177, 0.18431373],\n",
      "          [0.11764706, 0.16078432, 0.15686275, ..., 0.23921569, 0.14117648, 0.15294118],\n",
      "          [0.07450981, 0.08235294, 0.11764706, ..., 0.31372550, 0.17254902, 0.18431373],\n",
      "          ...,\n",
      "          [0.04313726, 0.04705882, 0.03921569, ..., 0.02745098, 0.02745098, 0.02745098],\n",
      "          [0.03921569, 0.04313726, 0.03529412, ..., 0.03137255, 0.03137255, 0.03137255],\n",
      "          [0.03529412, 0.03921569, 0.03529412, ..., 0.03137255, 0.03137255, 0.03137255]]],\n",
      "\n",
      "\n",
      "        [[[0.09411765, 0.09411765, 0.09019608, ..., 0.10196079, 0.10588235, 0.14117648],\n",
      "          [0.10980392, 0.09803922, 0.09411765, ..., 0.10980392, 0.14509805, 0.16862746],\n",
      "          [0.14901961, 0.10980392, 0.09411765, ..., 0.16078432, 0.24313726, 0.26274511],\n",
      "          ...,\n",
      "          [0.46274510, 0.45882353, 0.45490196, ..., 0.76470590, 0.76470590, 0.76470590],\n",
      "          [0.45882353, 0.45490196, 0.45098040, ..., 0.76078433, 0.76078433, 0.76078433],\n",
      "          [0.45490196, 0.45098040, 0.44705883, ..., 0.75686276, 0.75686276, 0.75686276]],\n",
      "\n",
      "         [[0.07450981, 0.07450981, 0.07058824, ..., 0.08235294, 0.09019608, 0.13333334],\n",
      "          [0.09019608, 0.07843138, 0.07450981, ..., 0.09411765, 0.13333334, 0.16078432],\n",
      "          [0.12941177, 0.09019608, 0.07450981, ..., 0.15294118, 0.23921569, 0.26274511],\n",
      "          ...,\n",
      "          [0.43921569, 0.43529412, 0.43137255, ..., 0.67450982, 0.67450982, 0.67450982],\n",
      "          [0.43529412, 0.43137255, 0.42745098, ..., 0.67058825, 0.67058825, 0.67058825],\n",
      "          [0.43137255, 0.42745098, 0.42352942, ..., 0.66666669, 0.66666669, 0.66666669]],\n",
      "\n",
      "         [[0.06274510, 0.06274510, 0.05882353, ..., 0.07843138, 0.09411765, 0.13725491],\n",
      "          [0.07843138, 0.06666667, 0.06274510, ..., 0.09019608, 0.13725491, 0.16862746],\n",
      "          [0.11764706, 0.07843138, 0.06274510, ..., 0.15686275, 0.24705882, 0.27058825],\n",
      "          ...,\n",
      "          [0.48627451, 0.48235294, 0.47843137, ..., 0.70588237, 0.70588237, 0.70588237],\n",
      "          [0.48235294, 0.47843137, 0.47450981, ..., 0.70196080, 0.70196080, 0.70196080],\n",
      "          [0.47843137, 0.47450981, 0.47058824, ..., 0.69803923, 0.69803923, 0.69803923]]],\n",
      "\n",
      "\n",
      "        [[[0.10980392, 0.10980392, 0.10980392, ..., 0.09803922, 0.10588235, 0.14117648],\n",
      "          [0.12156863, 0.11372549, 0.10588235, ..., 0.11372549, 0.14509805, 0.16470589],\n",
      "          [0.16078432, 0.11764706, 0.10588235, ..., 0.16078432, 0.23529412, 0.25098041],\n",
      "          ...,\n",
      "          [0.60000002, 0.60000002, 0.60000002, ..., 0.79607844, 0.80392158, 0.80000001],\n",
      "          [0.59607846, 0.59607846, 0.59607846, ..., 0.79215688, 0.80000001, 0.80000001],\n",
      "          [0.59215689, 0.59215689, 0.59215689, ..., 0.78431374, 0.79607844, 0.79607844]],\n",
      "\n",
      "         [[0.06274510, 0.06274510, 0.06274510, ..., 0.07843138, 0.09019608, 0.13333334],\n",
      "          [0.07843138, 0.06666667, 0.06274510, ..., 0.09803922, 0.13333334, 0.15686275],\n",
      "          [0.12156863, 0.08235294, 0.07058824, ..., 0.15294118, 0.23137255, 0.24705882],\n",
      "          ...,\n",
      "          [0.47843137, 0.47843137, 0.47843137, ..., 0.66666669, 0.67450982, 0.67058825],\n",
      "          [0.47450981, 0.47450981, 0.47450981, ..., 0.66274512, 0.67058825, 0.67058825],\n",
      "          [0.47058824, 0.47058824, 0.47058824, ..., 0.65490198, 0.66666669, 0.66666669]],\n",
      "\n",
      "         [[0.06274510, 0.06274510, 0.06274510, ..., 0.08627451, 0.10588235, 0.14901961],\n",
      "          [0.07843138, 0.06666667, 0.06274510, ..., 0.10588235, 0.14509805, 0.17254902],\n",
      "          [0.11764706, 0.07450981, 0.06666667, ..., 0.15686275, 0.23921569, 0.26274511],\n",
      "          ...,\n",
      "          [0.50980395, 0.50980395, 0.50980395, ..., 0.73333335, 0.74117649, 0.73725492],\n",
      "          [0.50588238, 0.50588238, 0.50588238, ..., 0.72941178, 0.73725492, 0.73725492],\n",
      "          [0.50196081, 0.50196081, 0.50196081, ..., 0.72156864, 0.73333335, 0.73333335]]]])\n",
      "Tensor(shape=[128], dtype=int64, place=CPUPlace, stop_gradient=True,\n",
      "       [0, 0, 2, 2, 1, 2, 2, 1, 0, 0, 0, 1, 0, 0, 3, 2, 0, 0, 0, 0, 1, 1, 0, 2, 2, 2, 3, 2, 0, 1, 2, 0, 0, 0, 1, 2, 1, 0, 0, 1, 0, 1, 2, 0, 1, 1, 0, 0, 3, 0, 2, 0, 3, 3, 2, 2, 1, 3, 3, 2, 1, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 3, 0, 0, 3, 0, 0, 2, 0, 0, 3, 0, 3, 3, 1, 2, 3, 0, 0, 0, 1, 0, 2, 0, 1, 2, 3, 3, 3, 2, 0, 0, 0, 1, 2, 0, 2, 3, 0, 0, 0, 0, 2, 0, 1, 0, 1, 3, 0, 1, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print('=============train dataset=============')\n",
    "for image, label in train_dataset:\n",
    "    print('image shape: {}, label: {}'.format(image.shape, label))\n",
    "    break\n",
    "\n",
    "for batch_id, data in enumerate(train_loader()):\n",
    "    x_data = data[0]\n",
    "    y_data = data[1]\n",
    "    print(x_data)\n",
    "    print(y_data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id           train_0000.jpg\n",
      "class_num                 0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "## 选择自己的baseline\n",
    "理想情况中，模型越大拟合能力越强。图像尺寸越大，保留的信息也越多，在实际情况中模型越复杂训练\n",
    "时间越长，图像越长尺寸越大训练时间也越长\n",
    "\n",
    "比赛开始有限使用最简单的resnet，快速跑完整个训练和预测流程，分类模型的选择需要根据任务复杂度来\n",
    "进行选择，并不是精度越高的模型月适合参加比赛\n",
    "\n",
    "在实际的比赛中可以逐步增加尺寸，在64-64的尺寸下让模型收敛，进而将模型放到128-128的尺寸爱训练\n",
    "\n",
    "在选择的过程中baseline应该遵循几点原则\n",
    "1. 复杂度地，代码结构简单\n",
    "2. loss收敛正确，metric出现提升\n",
    "3. 迭代快速，没有很fancy的模型结构、loss function或者图像预处理方法之类的\n",
    "4. 需要编写正确并简单的测试脚本，能够提交submission之后获得正确的分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 64, 56, 56]\n",
      "---------------------------------------------------------------------------\n",
      " Layer (type)       Input Shape          Output Shape         Param #    \n",
      "===========================================================================\n",
      "   Conv2D-21     [[1, 3, 224, 224]]   [1, 64, 112, 112]        9,472     \n",
      "  MaxPool2D-5   [[1, 64, 112, 112]]    [1, 64, 56, 56]           0       \n",
      "   Conv2D-22     [[1, 64, 56, 56]]     [1, 64, 56, 56]        36,928     \n",
      "   Conv2D-23     [[1, 64, 56, 56]]     [1, 128, 28, 28]       73,856     \n",
      "   Conv2D-24     [[1, 128, 28, 28]]    [1, 256, 14, 14]       295,168    \n",
      "   Conv2D-25     [[1, 256, 14, 14]]     [1, 512, 7, 7]       1,180,160   \n",
      "  Flatten-59      [[1, 512, 7, 7]]        [1, 25088]             0       \n",
      "   Linear-7         [[1, 25088]]           [1, 64]           1,605,696   \n",
      "   Linear-8          [[1, 64]]              [1, 4]              260      \n",
      "===========================================================================\n",
      "Total params: 3,201,540\n",
      "Trainable params: 3,201,540\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 10.72\n",
      "Params size (MB): 12.21\n",
      "Estimated Total Size (MB): 23.51\n",
      "---------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_params': 3201540, 'trainable_params': 3201540}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequential形式组网\r\n",
    "class MyNet(paddle.nn.Layer):\r\n",
    "    def __init__(self, num_classes=4):\r\n",
    "        super(MyNet, self).__init__()\r\n",
    "        self.conv1 = paddle.nn.Conv2D(in_channels=3, out_channels=64, kernel_size=(7, 7), stride=2, padding = 3)\r\n",
    "        self.pool1 = paddle.nn.MaxPool2D(kernel_size=2, stride=2)\r\n",
    "\r\n",
    "        self.conv2 = paddle.nn.Conv2D(in_channels=64, out_channels=64, kernel_size=(3,3),  stride=1, padding = 1)\r\n",
    "        self.conv3 = paddle.nn.Conv2D(in_channels=64, out_channels=128, kernel_size=(3,3),  stride=2, padding = 1)\r\n",
    "        self.conv4 = paddle.nn.Conv2D(in_channels=128, out_channels=256, kernel_size=(3,3),  stride=2, padding = 1)\r\n",
    "        self.conv5 = paddle.nn.Conv2D(in_channels=256, out_channels=512, kernel_size=(3,3),  stride=2, padding = 1)\r\n",
    "        # # self.pool2 = paddle.nn.MaxPool2D(kernel_size=2, stride=2)\r\n",
    "\r\n",
    "        # self.conv3 = paddle.nn.Conv2D(in_channels=448, out_channels=448, kernel_size=(3,3), stride=2, padding = 0)\r\n",
    "\r\n",
    "        # self.conv4 = paddle.nn.Conv2D(in_channels=448, out_channels=448, kernel_size=(3,3), stride=2, padding = 1)\r\n",
    "\r\n",
    "        self.flatten = paddle.nn.Flatten()\r\n",
    "        self.linear1 = paddle.nn.Linear(in_features=25088, out_features=64)\r\n",
    "        self.linear2 = paddle.nn.Linear(in_features=64, out_features=num_classes)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.conv1(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.pool1(x)\r\n",
    "        print(x.shape)\r\n",
    "        x = self.conv2(x)\r\n",
    "        x = F.relu(x)\r\n",
    "\r\n",
    "        x = self.conv3(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        # # print(x.shape)\r\n",
    "        x = self.conv4(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.conv5(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        # x = self.conv4(x)\r\n",
    "        # x = F.relu(x)\r\n",
    "        # # print(x.shape)\r\n",
    "\r\n",
    "        x = self.flatten(x)\r\n",
    "        x = self.linear1(x)\r\n",
    "        x = F.relu(x)\r\n",
    "        x = self.linear2(x)\r\n",
    "        return x\r\n",
    "\r\n",
    "model = paddle.Model(MyNet())\r\n",
    "model.summary((1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`./vdl/vdlrecords.model.log` is exists, VisualDL will add logs to it.\n"
     ]
    }
   ],
   "source": [
    "# 注意过程中特征图的计算\n",
    "# 定义优化器\n",
    "optim = paddle.optimizer.Adam(learning_rate=0.001, parameters=model.parameters())\n",
    "model.prepare(\n",
    "    optim,\n",
    "    paddle.nn.CrossEntropyLoss(),\n",
    "    Accuracy()\n",
    "    )\n",
    "\n",
    "from visualdl import LogReader, LogWriter\n",
    "\n",
    "args={\n",
    "    'logdir':'./vdl',\n",
    "    'file_name':'vdlrecords.model.log',\n",
    "    'iters':0,\n",
    "}\n",
    "\n",
    "# 配置visualdl\n",
    "write = LogWriter(logdir=args['logdir'], file_name=args['file_name'])\n",
    "#iters 初始化为0\n",
    "iters = args['iters'] \n",
    "\n",
    "#自定义Callback\n",
    "class Callbk(paddle.callbacks.Callback):\n",
    "    def __init__(self, write, iters=0):\n",
    "        self.write = write\n",
    "        self.iters = iters\n",
    "\n",
    "    def on_train_batch_end(self, step, logs):\n",
    "\n",
    "        self.iters += 1\n",
    "\n",
    "        #记录loss\n",
    "        self.write.add_scalar(tag=\"loss\",step=self.iters,value=logs['loss'][0])\n",
    "        #记录 accuracy\n",
    "        self.write.add_scalar(tag=\"acc\",step=self.iters,value=logs['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous step.\n",
      "Epoch 1/15\n",
      "[128, 64, 56, 56]\n",
      "step 1/7 [===>..........................] - loss: 1.4849 - acc: 0.2266 - ETA: 7s - 1s/step[128, 64, 56, 56]\n",
      "step 2/7 [=======>......................] - loss: 6.7525 - acc: 0.2930 - ETA: 6s - 1s/step[128, 64, 56, 56]\n",
      "step 3/7 [===========>..................] - loss: 1.8031 - acc: 0.2708 - ETA: 4s - 1s/step[128, 64, 56, 56]\n",
      "step 4/7 [================>.............] - loss: 1.3808 - acc: 0.2480 - ETA: 3s - 1s/step[128, 64, 56, 56]\n",
      "step 5/7 [====================>.........] - loss: 1.2969 - acc: 0.2750 - ETA: 2s - 1s/step[128, 64, 56, 56]\n",
      "step 6/7 [========================>.....] - loss: 1.2021 - acc: 0.3164 - ETA: 1s - 1s/step[113, 64, 56, 56]\n",
      "step 7/7 [==============================] - loss: 1.1513 - acc: 0.3360 - 1s/step          \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "[128, 64, 56, 56]\n",
      "step 1/2 [==============>...............] - loss: 1.0067 - acc: 0.6797 - ETA: 1s - 1s/step[93, 64, 56, 56]\n",
      "step 2/2 [==============================] - loss: 1.0514 - acc: 0.6606 - 1s/step          \n",
      "Eval samples: 221\n",
      "Epoch 2/15\n",
      "[128, 64, 56, 56]\n",
      "step 1/7 [===>..........................] - loss: 0.9772 - acc: 0.7734 - ETA: 7s - 1s/step[128, 64, 56, 56]\n",
      "step 2/7 [=======>......................] - loss: 0.8334 - acc: 0.7734 - ETA: 6s - 1s/step[128, 64, 56, 56]\n",
      "step 3/7 [===========>..................] - loss: 0.6969 - acc: 0.7604 - ETA: 4s - 1s/step[128, 64, 56, 56]\n",
      "step 4/7 [================>.............] - loss: 2.3449 - acc: 0.6484 - ETA: 3s - 1s/step[128, 64, 56, 56]\n",
      "step 5/7 [====================>.........] - loss: 1.3612 - acc: 0.6516 - ETA: 2s - 1s/step[128, 64, 56, 56]\n",
      "step 6/7 [========================>.....] - loss: 1.5054 - acc: 0.6380 - ETA: 1s - 1s/step[113, 64, 56, 56]\n",
      "step 7/7 [==============================] - loss: 1.1878 - acc: 0.6436 - 1s/step          \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "[128, 64, 56, 56]\n",
      "step 1/2 [==============>...............] - loss: 0.9094 - acc: 0.7266 - ETA: 1s - 1s/step[93, 64, 56, 56]\n",
      "step 2/2 [==============================] - loss: 0.8898 - acc: 0.7195 - 1s/step          \n",
      "Eval samples: 221\n",
      "Epoch 3/15\n",
      "[128, 64, 56, 56]\n",
      "step 1/7 [===>..........................] - loss: 0.9101 - acc: 0.6641 - ETA: 7s - 1s/step[128, 64, 56, 56]\n",
      "step 2/7 [=======>......................] - loss: 0.8710 - acc: 0.6836 - ETA: 6s - 1s/step[128, 64, 56, 56]\n",
      "step 3/7 [===========>..................] - loss: 0.8483 - acc: 0.6875 - ETA: 4s - 1s/step[128, 64, 56, 56]\n",
      "step 4/7 [================>.............] - loss: 0.8448 - acc: 0.7031 - ETA: 3s - 1s/step[128, 64, 56, 56]\n",
      "step 5/7 [====================>.........] - loss: 0.8475 - acc: 0.7063 - ETA: 2s - 1s/step[128, 64, 56, 56]\n",
      "step 6/7 [========================>.....] - loss: 0.7537 - acc: 0.7122 - ETA: 1s - 1s/step[113, 64, 56, 56]\n",
      "step 7/7 [==============================] - loss: 0.6307 - acc: 0.7185 - 1s/step          \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "[128, 64, 56, 56]\n",
      "step 1/2 [==============>...............] - loss: 0.6840 - acc: 0.7891 - ETA: 1s - 1s/step[93, 64, 56, 56]\n",
      "step 2/2 [==============================] - loss: 0.6577 - acc: 0.7783 - 1s/step          \n",
      "Eval samples: 221\n",
      "Epoch 4/15\n",
      "[128, 64, 56, 56]\n",
      "step 1/7 [===>..........................] - loss: 0.7030 - acc: 0.7344 - ETA: 7s - 1s/step[128, 64, 56, 56]\n",
      "step 2/7 [=======>......................] - loss: 0.4887 - acc: 0.8203 - ETA: 6s - 1s/step[128, 64, 56, 56]\n",
      "step 3/7 [===========>..................] - loss: 0.5533 - acc: 0.8333 - ETA: 4s - 1s/step[128, 64, 56, 56]\n",
      "step 4/7 [================>.............] - loss: 0.4893 - acc: 0.8438 - ETA: 3s - 1s/step[128, 64, 56, 56]\n",
      "step 5/7 [====================>.........] - loss: 0.5349 - acc: 0.8375 - ETA: 2s - 1s/step[128, 64, 56, 56]\n",
      "step 6/7 [========================>.....] - loss: 0.3818 - acc: 0.8411 - ETA: 1s - 1s/step[113, 64, 56, 56]\n",
      "step 7/7 [==============================] - loss: 0.2702 - acc: 0.8490 - 1s/step          \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "[128, 64, 56, 56]\n",
      "step 1/2 [==============>...............] - loss: 0.5461 - acc: 0.8125 - ETA: 1s - 1s/step[93, 64, 56, 56]\n",
      "step 2/2 [==============================] - loss: 0.3558 - acc: 0.8416 - 1s/step          \n",
      "Eval samples: 221\n",
      "Epoch 5/15\n",
      "[128, 64, 56, 56]\n",
      "step 1/7 [===>..........................] - loss: 0.4032 - acc: 0.8281 - ETA: 7s - 1s/step[128, 64, 56, 56]\n",
      "step 2/7 [=======>......................] - loss: 0.2782 - acc: 0.8828 - ETA: 6s - 1s/step[128, 64, 56, 56]\n",
      "step 3/7 [===========>..................] - loss: 0.2537 - acc: 0.8932 - ETA: 4s - 1s/step[128, 64, 56, 56]\n",
      "step 4/7 [================>.............] - loss: 0.3106 - acc: 0.9004 - ETA: 3s - 1s/step[128, 64, 56, 56]\n",
      "step 5/7 [====================>.........] - loss: 0.3652 - acc: 0.8953 - ETA: 2s - 1s/step[128, 64, 56, 56]\n",
      "step 6/7 [========================>.....] - loss: 0.2224 - acc: 0.8984 - ETA: 1s - 1s/step[113, 64, 56, 56]\n",
      "step 7/7 [==============================] - loss: 0.3256 - acc: 0.8990 - 1s/step          \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "[128, 64, 56, 56]\n",
      "step 1/2 [==============>...............] - loss: 0.3408 - acc: 0.8438 - ETA: 1s - 1s/step[93, 64, 56, 56]\n",
      "step 2/2 [==============================] - loss: 0.3114 - acc: 0.8326 - 1s/step          \n",
      "Eval samples: 221\n",
      "Epoch 6/15\n",
      "[128, 64, 56, 56]\n",
      "step 1/7 [===>..........................] - loss: 0.2124 - acc: 0.9062 - ETA: 7s - 1s/step[128, 64, 56, 56]\n",
      "step 2/7 [=======>......................] - loss: 0.1708 - acc: 0.9258 - ETA: 6s - 1s/step[128, 64, 56, 56]\n",
      "step 3/7 [===========>..................] - loss: 0.2870 - acc: 0.9167 - ETA: 4s - 1s/step[128, 64, 56, 56]\n",
      "step 4/7 [================>.............] - loss: 0.1881 - acc: 0.9180 - ETA: 3s - 1s/step[128, 64, 56, 56]\n",
      "step 5/7 [====================>.........] - loss: 0.3036 - acc: 0.9156 - ETA: 2s - 1s/step[128, 64, 56, 56]\n",
      "step 6/7 [========================>.....] - loss: 0.3152 - acc: 0.9089 - ETA: 1s - 1s/step[113, 64, 56, 56]\n",
      "step 7/7 [==============================] - loss: 0.1624 - acc: 0.9092 - 1s/step          \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "[128, 64, 56, 56]\n",
      "step 1/2 [==============>...............] - loss: 0.3151 - acc: 0.9141 - ETA: 1s - 1s/step[93, 64, 56, 56]\n",
      "step 2/2 [==============================] - loss: 0.3244 - acc: 0.9050 - 1s/step          \n",
      "Eval samples: 221\n",
      "Epoch 7/15\n",
      "[128, 64, 56, 56]\n",
      "step 1/7 [===>..........................] - loss: 0.4050 - acc: 0.8750 - ETA: 7s - 1s/step[128, 64, 56, 56]\n",
      "step 2/7 [=======>......................] - loss: 0.1410 - acc: 0.9023 - ETA: 6s - 1s/step[128, 64, 56, 56]\n",
      "step 3/7 [===========>..................] - loss: 0.2465 - acc: 0.9089 - ETA: 4s - 1s/step[128, 64, 56, 56]\n",
      "step 4/7 [================>.............] - loss: 0.2190 - acc: 0.9121 - ETA: 3s - 1s/step[128, 64, 56, 56]\n",
      "step 5/7 [====================>.........] - loss: 0.1823 - acc: 0.9109 - ETA: 2s - 1s/step[128, 64, 56, 56]\n",
      "step 6/7 [========================>.....] - loss: 0.2198 - acc: 0.9102 - ETA: 1s - 1s/step[113, 64, 56, 56]\n",
      "step 7/7 [==============================] - loss: 0.2194 - acc: 0.9115 - 1s/step          \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "[128, 64, 56, 56]\n",
      "step 1/2 [==============>...............] - loss: 0.2905 - acc: 0.8672 - ETA: 1s - 1s/step[93, 64, 56, 56]\n",
      "step 2/2 [==============================] - loss: 0.2588 - acc: 0.8733 - 1s/step          \n",
      "Eval samples: 221\n",
      "Epoch 8/15\n",
      "[128, 64, 56, 56]\n",
      "step 1/7 [===>..........................] - loss: 0.1634 - acc: 0.9062 - ETA: 7s - 1s/step[128, 64, 56, 56]\n",
      "step 2/7 [=======>......................] - loss: 0.1506 - acc: 0.9219 - ETA: 6s - 1s/step[128, 64, 56, 56]\n",
      "step 3/7 [===========>..................] - loss: 0.1242 - acc: 0.9297 - ETA: 4s - 1s/step[128, 64, 56, 56]\n",
      "step 4/7 [================>.............] - loss: 0.3063 - acc: 0.9199 - ETA: 3s - 1s/step[128, 64, 56, 56]\n",
      "step 5/7 [====================>.........] - loss: 0.1786 - acc: 0.9219 - ETA: 2s - 1s/step[128, 64, 56, 56]\n",
      "step 6/7 [========================>.....] - loss: 0.1867 - acc: 0.9232 - ETA: 1s - 1s/step[113, 64, 56, 56]\n",
      "step 7/7 [==============================] - loss: 0.1735 - acc: 0.9262 - 1s/step          \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "[128, 64, 56, 56]\n",
      "step 1/2 [==============>...............] - loss: 0.2529 - acc: 0.8750 - ETA: 1s - 1s/step[93, 64, 56, 56]\n",
      "step 2/2 [==============================] - loss: 0.2749 - acc: 0.8688 - 1s/step          \n",
      "Eval samples: 221\n",
      "Epoch 9/15\n",
      "[128, 64, 56, 56]\n",
      "step 1/7 [===>..........................] - loss: 0.1865 - acc: 0.9141 - ETA: 7s - 1s/step[128, 64, 56, 56]\n",
      "step 2/7 [=======>......................] - loss: 0.2470 - acc: 0.9023 - ETA: 6s - 1s/step[128, 64, 56, 56]\n",
      "step 3/7 [===========>..................] - loss: 0.0620 - acc: 0.9297 - ETA: 4s - 1s/step[128, 64, 56, 56]\n",
      "step 4/7 [================>.............] - loss: 0.1055 - acc: 0.9375 - ETA: 3s - 1s/step[128, 64, 56, 56]\n",
      "step 5/7 [====================>.........] - loss: 0.2145 - acc: 0.9344 - ETA: 2s - 1s/step[128, 64, 56, 56]\n",
      "step 6/7 [========================>.....] - loss: 0.0945 - acc: 0.9388 - ETA: 1s - 1s/step[113, 64, 56, 56]\n",
      "step 7/7 [==============================] - loss: 0.1622 - acc: 0.9387 - 1s/step          \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "[128, 64, 56, 56]\n",
      "step 1/2 [==============>...............] - loss: 0.2351 - acc: 0.8906 - ETA: 1s - 1s/step[93, 64, 56, 56]\n",
      "step 2/2 [==============================] - loss: 0.2440 - acc: 0.9005 - 1s/step          \n",
      "Eval samples: 221\n",
      "Epoch 10/15\n",
      "[128, 64, 56, 56]\n",
      "step 1/7 [===>..........................] - loss: 0.1562 - acc: 0.9375 - ETA: 7s - 1s/step[128, 64, 56, 56]\n",
      "step 2/7 [=======>......................] - loss: 0.1392 - acc: 0.9531 - ETA: 6s - 1s/step[128, 64, 56, 56]\n",
      "step 3/7 [===========>..................] - loss: 0.1352 - acc: 0.9479 - ETA: 4s - 1s/step[128, 64, 56, 56]\n",
      "step 4/7 [================>.............] - loss: 0.0883 - acc: 0.9551 - ETA: 3s - 1s/step[128, 64, 56, 56]\n",
      "step 5/7 [====================>.........] - loss: 0.1463 - acc: 0.9531 - ETA: 2s - 1s/step[128, 64, 56, 56]\n",
      "step 6/7 [========================>.....] - loss: 0.1493 - acc: 0.9505 - ETA: 1s - 1s/step[113, 64, 56, 56]\n",
      "step 7/7 [==============================] - loss: 0.1267 - acc: 0.9523 - 1s/step          \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "[128, 64, 56, 56]\n",
      "step 1/2 [==============>...............] - loss: 0.2258 - acc: 0.9062 - ETA: 1s - 1s/step[93, 64, 56, 56]\n",
      "step 2/2 [==============================] - loss: 0.1954 - acc: 0.9186 - 1s/step          \n",
      "Eval samples: 221\n",
      "Epoch 11/15\n",
      "[128, 64, 56, 56]\n",
      "step 1/7 [===>..........................] - loss: 0.1226 - acc: 0.9531 - ETA: 7s - 1s/step[128, 64, 56, 56]\n",
      "step 2/7 [=======>......................] - loss: 0.1305 - acc: 0.9453 - ETA: 6s - 1s/step[128, 64, 56, 56]\n",
      "step 3/7 [===========>..................] - loss: 0.1340 - acc: 0.9453 - ETA: 4s - 1s/step[128, 64, 56, 56]\n",
      "step 4/7 [================>.............] - loss: 0.0711 - acc: 0.9551 - ETA: 3s - 1s/step[128, 64, 56, 56]\n",
      "step 5/7 [====================>.........] - loss: 0.0544 - acc: 0.9609 - ETA: 2s - 1s/step[128, 64, 56, 56]\n",
      "step 6/7 [========================>.....] - loss: 0.1100 - acc: 0.9596 - ETA: 1s - 1s/step[113, 64, 56, 56]\n",
      "step 7/7 [==============================] - loss: 0.1838 - acc: 0.9569 - 1s/step          \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "[128, 64, 56, 56]\n",
      "step 1/2 [==============>...............] - loss: 0.1604 - acc: 0.9531 - ETA: 1s - 1s/step[93, 64, 56, 56]\n",
      "step 2/2 [==============================] - loss: 0.2993 - acc: 0.9367 - 1s/step          \n",
      "Eval samples: 221\n",
      "Epoch 12/15\n",
      "[128, 64, 56, 56]\n",
      "step 1/7 [===>..........................] - loss: 0.1236 - acc: 0.9453 - ETA: 7s - 1s/step[128, 64, 56, 56]\n",
      "step 2/7 [=======>......................] - loss: 0.0547 - acc: 0.9688 - ETA: 6s - 1s/step[128, 64, 56, 56]\n",
      "step 3/7 [===========>..................] - loss: 0.2062 - acc: 0.9505 - ETA: 4s - 1s/step[128, 64, 56, 56]\n",
      "step 4/7 [================>.............] - loss: 0.1316 - acc: 0.9492 - ETA: 3s - 1s/step[128, 64, 56, 56]\n",
      "step 5/7 [====================>.........] - loss: 0.0759 - acc: 0.9563 - ETA: 2s - 1s/step[128, 64, 56, 56]\n",
      "step 6/7 [========================>.....] - loss: 0.0801 - acc: 0.9609 - ETA: 1s - 1s/step[113, 64, 56, 56]\n",
      "step 7/7 [==============================] - loss: 0.2175 - acc: 0.9580 - 1s/step          \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "[128, 64, 56, 56]\n",
      "step 1/2 [==============>...............] - loss: 0.2175 - acc: 0.9219 - ETA: 1s - 1s/step[93, 64, 56, 56]\n",
      "step 2/2 [==============================] - loss: 0.1687 - acc: 0.9367 - 1s/step          \n",
      "Eval samples: 221\n",
      "Epoch 13/15\n",
      "[128, 64, 56, 56]\n",
      "step 1/7 [===>..........................] - loss: 0.1188 - acc: 0.9531 - ETA: 7s - 1s/step[128, 64, 56, 56]\n",
      "step 2/7 [=======>......................] - loss: 0.0690 - acc: 0.9609 - ETA: 6s - 1s/step[128, 64, 56, 56]\n",
      "step 3/7 [===========>..................] - loss: 0.1382 - acc: 0.9557 - ETA: 4s - 1s/step[128, 64, 56, 56]\n",
      "step 4/7 [================>.............] - loss: 0.1006 - acc: 0.9551 - ETA: 3s - 1s/step[128, 64, 56, 56]\n",
      "step 5/7 [====================>.........] - loss: 0.0434 - acc: 0.9625 - ETA: 2s - 1s/step[128, 64, 56, 56]\n",
      "step 6/7 [========================>.....] - loss: 0.0742 - acc: 0.9635 - ETA: 1s - 1s/step[113, 64, 56, 56]\n",
      "step 7/7 [==============================] - loss: 0.1249 - acc: 0.9637 - 1s/step          \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "[128, 64, 56, 56]\n",
      "step 1/2 [==============>...............] - loss: 0.1632 - acc: 0.9453 - ETA: 1s - 1s/step[93, 64, 56, 56]\n",
      "step 2/2 [==============================] - loss: 0.2240 - acc: 0.9412 - 1s/step          \n",
      "Eval samples: 221\n",
      "Epoch 14/15\n",
      "[128, 64, 56, 56]\n",
      "step 1/7 [===>..........................] - loss: 0.0547 - acc: 0.9766 - ETA: 7s - 1s/step[128, 64, 56, 56]\n",
      "step 2/7 [=======>......................] - loss: 0.0427 - acc: 0.9844 - ETA: 6s - 1s/step[128, 64, 56, 56]\n",
      "step 3/7 [===========>..................] - loss: 0.0513 - acc: 0.9818 - ETA: 4s - 1s/step[128, 64, 56, 56]\n",
      "step 4/7 [================>.............] - loss: 0.0545 - acc: 0.9824 - ETA: 3s - 1s/step[128, 64, 56, 56]\n",
      "step 5/7 [====================>.........] - loss: 0.0608 - acc: 0.9812 - ETA: 2s - 1s/step[128, 64, 56, 56]\n",
      "step 6/7 [========================>.....] - loss: 0.0513 - acc: 0.9831 - ETA: 1s - 1s/step[113, 64, 56, 56]\n",
      "step 7/7 [==============================] - loss: 0.0402 - acc: 0.9852 - 1s/step          \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "[128, 64, 56, 56]\n",
      "step 1/2 [==============>...............] - loss: 0.1093 - acc: 0.9531 - ETA: 1s - 1s/step[93, 64, 56, 56]\n",
      "step 2/2 [==============================] - loss: 0.2143 - acc: 0.9548 - 1s/step          \n",
      "Eval samples: 221\n",
      "Epoch 15/15\n",
      "[128, 64, 56, 56]\n",
      "step 1/7 [===>..........................] - loss: 0.0608 - acc: 0.9766 - ETA: 7s - 1s/step[128, 64, 56, 56]\n",
      "step 2/7 [=======>......................] - loss: 0.0476 - acc: 0.9805 - ETA: 5s - 1s/step[128, 64, 56, 56]\n",
      "step 3/7 [===========>..................] - loss: 0.0400 - acc: 0.9844 - ETA: 4s - 1s/step[128, 64, 56, 56]\n",
      "step 4/7 [================>.............] - loss: 0.0271 - acc: 0.9863 - ETA: 3s - 1s/step[128, 64, 56, 56]\n",
      "step 5/7 [====================>.........] - loss: 0.0198 - acc: 0.9891 - ETA: 2s - 1s/step[128, 64, 56, 56]\n",
      "step 6/7 [========================>.....] - loss: 0.0367 - acc: 0.9896 - ETA: 1s - 1s/step[113, 64, 56, 56]\n",
      "step 7/7 [==============================] - loss: 0.0152 - acc: 0.9909 - 1s/step          \n",
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "[128, 64, 56, 56]\n",
      "step 1/2 [==============>...............] - loss: 0.1904 - acc: 0.9609 - ETA: 1s - 1s/step[93, 64, 56, 56]\n",
      "step 2/2 [==============================] - loss: 0.2047 - acc: 0.9502 - 1s/step          \n",
      "Eval samples: 221\n"
     ]
    }
   ],
   "source": [
    "# 模型训练与评估\r\n",
    "model.fit(train_loader,\r\n",
    "        val_loader,\r\n",
    "        log_freq=1,\r\n",
    "        epochs=15,\r\n",
    "        callbacks=Callbk(write=write, iters=iters),\r\n",
    "        verbose=1, \r\n",
    "        )\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval begin...\n",
      "The loss value printed in the log is the current batch, and the metric is the average value of previous step.\n",
      "[128, 64, 56, 56]\n",
      "step 1/2 [==============>...............] - loss: 0.0984 - acc: 0.9688 - ETA: 1s - 1s/step[93, 64, 56, 56]\n",
      "step 2/2 [==============================] - loss: 0.3313 - acc: 0.9502 - 1s/step          \n",
      "Eval samples: 221\n",
      "{'loss': [0.33130556], 'acc': 0.9502262443438914}\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(val_loader,batch_size=32,log_freq=1, verbose=1, num_workers=0, callbacks=None)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
